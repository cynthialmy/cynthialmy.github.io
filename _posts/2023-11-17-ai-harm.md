---
layout: post
title: Mitigating Potential Harms in Generative AI Solutions
subtitle: "The Four-Layered Approach for AI Safety"
tags: [LLM, AI, Safety, Generative, OpenAI, Azure, LangChain, RAG, GPT-4]
comments: true
author: Cynthia Mengyuan Li
---

I've had the opportunity to work with various tools like LangChain and OpenAI API, developing applications, creating vector stores, and fine-tuning models. My journey led me to explore Microsoft's Azure OpenAI Service, which offered a wealth of insights, particularly in mitigating potential harms of generative AI solutions.

## Project Overview

The procurement AI project aimed to streamline the procurement process at Volvo Cars by integrating Gen AI to facilitate contract management and decision-making. The current system, involving VGS, VPC, and SI+ systems, is fragmented and manual, leading to inefficiencies and risks. Our solution proposes a cohesive AI-driven approach to enhance accuracy, compliance, and efficiency.

### The Importance of Limiting AI Harm

Generative AI, while innovative, comes with challenges, particularly around the potential for generating harmful or inappropriate content. Ensuring that AI solutions are safe and reliable is crucial, especially in sensitive areas like procurement where inaccuracies or misuse can lead to significant financial and legal consequences. In the procurement AI project, mitigating these potential harms was a priority to ensure the system's reliability and user trust.

**Why Limit AI Harm:**
1. **Accuracy and Compliance:** Ensuring AI-generated information is accurate and complies with legal standards is critical.
2. **User Trust:** Users need to trust the AI system to adopt it fully.
3. **Risk Mitigation:** Reducing the risk of generating harmful content protects the organization from potential legal and financial repercussions.

## The Four-Layered Approach to Harm Mitigation

Generative AI solutions require a structured approach to tackle potential harms effectively. Here’s how we applied this approach in our procurement AI project:

### 1. The Model Layer

At the heart of any generative AI solution is the model. Selecting an appropriate model for your solution is crucial. In our procurement AI project, while GPT-4 was powerful, a simpler model might have sufficed for specific tasks, reducing the risk of generating harmful content. Fine-tuning our model with specific procurement training data ensured more relevant and safe outputs.

**Example:**
- **Problem Identified:** The risk of the AI model generating inaccurate or irrelevant procurement contract details.
- **Solution:** Fine-tune GPT-4 with procurement-specific data, ensuring it understands the nuances of procurement contracts and related queries.

### 2. The Safety System Layer

This layer involves platform-level configurations and capabilities that help mitigate harm. For the procurement AI project, we utilized Azure OpenAI Service’s content filters to manage the severity levels of content. Implementing abuse detection algorithms and alert systems was crucial to prevent misuse and promptly respond to any harmful behavior.

**Example:**
- **Problem Identified:** Potential for the AI to generate incorrect or inappropriate responses to procurement queries.
- **Solution:** Implement content filters and abuse detection systems to monitor and flag inappropriate content.

### 3. The Metaprompt and Grounding Layer

Constructing prompts submitted to the model is critical. We used metaprompts to define behavioral parameters and applied prompt engineering to ensure relevant and nonharmful outputs. Retrieval augmented generation (RAG) was employed to pull contextual data from trusted sources, enhancing prompt quality and safety.

**Example:**
- **Problem Identified:** Ambiguity in user queries leading to potentially harmful or irrelevant AI responses.
- **Solution:** Use metaprompts and RAG to provide the AI with clear, contextually relevant instructions, pulling data from reliable sources to ground its responses.

### 4. The User Experience Layer

This layer encompasses the application interface and user documentation. In our project, designing user interfaces that limited inputs to specific subjects and types, along with validating inputs and outputs, significantly reduced the risk of harmful responses. Transparent documentation about the system’s capabilities and limitations was also crucial.

**Example:**
- **Problem Identified:** Users inadvertently inputting vague or broad queries that could lead to misleading AI responses.
- **Solution:** Design a user interface that guides users to input precise queries and provide thorough documentation on the system's use and limitations.

## User Study and Implementation

Conducting a thorough user study was vital in identifying and addressing potential problems:
- **User Onboarding:** We onboarded users to understand how to interact with the AI tool, emphasizing prompt construction.
- **Controlled Experiments:** Initial phase involved structured scenario-based testing to ensure basic functionalities and safety measures were effective.
- **Exploratory Testing:** Users engaged with the AI in an open-ended manner to provide feedback on usability and potential harms.

**Findings:**
- Users initially struggled with prompt specificity, leading to irrelevant outputs. Training improved this.
- Content filters effectively flagged and managed inappropriate responses, enhancing safety.
- Feedback highlighted the importance of clear documentation and user guidance to prevent misuse.
